## Apache Spark简介

Apache Spark是一种快速的集群计算技术，专为快速计算而设计。它基于Hadoop MapReduce，它扩展了MapReduce模型，以有效地将其用于更多类型的计算，包括交互式查询和流处理。 Spark的主要特性是它的**内存中集群计算**，提高了应用程序的处理速度。
Spark旨在涵盖各种工作负载，如批处理应用程序，迭代算法，交互式查询和流式处理。除了在相应系统中支持所有这些工作负载之外，它还减少了维护单独工具的管理负担。

## Apache Spark的特性

Apache Spark具有以下功能。

**速度**

Spark有助于在Hadoop集群中运行应用程序，在内存中速度提高100倍，在磁盘上运行时提高10倍。这可以通过减少对磁盘的读/写操作的数量来实现。它将中间处理数据存储在存储器中。

**支持多种语言**

Spark在Java，Scala或Python中提供了内置的API。因此，您可以使用不同的语言编写应用程序。 Spark提供了80个高级操作符进行交互式查询。

**高级分析**

Spark不仅支持“Map”和“reduce”。它还支持SQL查询，流数据，机器学习（ML）和图算法。

## Spark基于Hadoop

下图显示了如何使用Hadoop组件构建Spark的三种方式。



![星火内置在Hadoop](..\imgs\大数据\spark_built_on_hadoop.jpg)



Spark部署有三种方式，如下所述。
**Standalone**- Spark独立部署意味着Spark占据HDFS（Hadoop分布式文件系统）顶部的位置，并明确为HDFS分配空间。 这里，Spark和MapReduce将并行运行以覆盖集群上的所有spark作业。
**Hadoop Yarn**- Hadoop Yarn部署意味着，spark只需运行在Yarn上，无需任何预安装或根访问。 它有助于将Spark集成到Hadoop生态系统或Hadoop堆栈中。 它允许其他组件在堆栈顶部运行。
**Spark in MapReduce (SIMR)** - MapReduce中的Spark用于在独立部署之外启动spark job。 使用SIMR，用户可以启动Spark并使用其shell而无需任何管理访问。



## Spark的组件

下图说明了Spark的不同组件。

![星火组件](..\imgs\大数据\components_of_spark.jpg)

### 

### Apache Spark Core

Spark Core是spark平台的基础通用执行引擎，所有其他功能都是基于。它在外部存储系统中提供内存计算和引用数据集。
**Spark SQL**
Spark SQL是Spark Core之上的一个组件，它引入了一个称为SchemaRDD的新数据抽象，它为结构化和半结构化数据提供支持。

**Spark Streaming**

Spark Streaming利用Spark Core的快速调度功能来执行流式分析。它以小批量获取数据，并对这些小批量的数据执行RDD（弹性分布式数据集）转换。

**MLlib (Machine Learning Library)**

MLlib是Spark之上的分布式机器学习框架，因为基于分布式内存的Spark架构。根据基准，它是由MLlib开发人员针对交替最小二乘法（ALS）实现完成的。 Spark MLlib是基于Hadoop磁盘的**Apache Mahout**版本的9倍（在Mahout获得了Spark接口之前）。
**GraphX**
GraphX是Spark上的一个分布式图形处理框架。它提供了一个用于表达图形计算的API，可以通过使用Pregel抽象API为用户定义的图形建模。它还为此抽象提供了一个优化的运行时。


